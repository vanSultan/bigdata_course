{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if \"HADOOP_CONF_DIR\" in os.environ:\n",
    "   del os.environ[\"HADOOP_CONF_DIR\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "\"HADOOP_CONF_DIR\" in os.environ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Web UI: http://10.128.251.168:4040\n",
      "\n",
      "log4j file: /home/jovyan/conf/pyspark-log4j-producer_app.properties\n",
      "\n",
      "driver log file: /home/jovyan/logs/pyspark-producer_app.log\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, length, when, col\n",
    "from pyspark.sql.types import BooleanType, IntegerType, LongType, StringType, ArrayType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "\n",
    "# setting constants\n",
    "APP_NAME = \"producer_app\"\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_')\n",
    "\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = \"-Dlog4j.configuration=file://{} -Dspark.hadoop.dfs.replication=1 -Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\\\n",
    "    .format(LOG4J_PROP_FILE)\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "# preparing configuration files from templates\n",
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template\\\n",
    "    .stream(logfile=LOG_FILE)\\\n",
    "    .dump(LOG4J_PROP_FILE)\n",
    "\n",
    "# run spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(APP_NAME)\\\n",
    "    .master(\"k8s://https://10.32.7.103:6443\")\\\n",
    "    .config(\"spark.driver.host\", LOCAL_IP)\\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", '3')\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.6\")\\\n",
    "    .config(\"spark.executor.memory\", '3g')\\\n",
    "    .config(\"spark.driver.memory\", \"3g\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "    .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\\\n",
    "    .config(\"spark.kubernetes.namespace\", \"szhumabaev-307617\")\\\n",
    "    .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.container.image\", \"node03.st:5000/spark-executor:szhumabaev-307617\")\\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.path\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.readOnly\", \"false\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.path\", \"/home/jovyan/shared-data\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.path\", \"/nfs/shared\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.type\", \"Directory\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.readOnly\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# printing important urls and pathes\n",
    "print(\"Web UI: {}\".format(spark.sparkContext.uiWebUrl))\n",
    "print(\"\\nlog4j file: {}\".format(LOG4J_PROP_FILE))\n",
    "print(\"\\ndriver log file: {}\".format(LOG_FILE))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from kafka import KafkaProducer\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-------+--------------------+----------+\n",
      "|user_id|                text| timestamp|\n",
      "+-------+--------------------+----------+\n",
      "|  57114|Я всё время себя ...|1552061847|\n",
      "|  57114|Я наверно не тот ...|1552066470|\n",
      "|  57114|Местами дождь, ме...|1552223531|\n",
      "|  57114|Ну отчего все так...|1552250945|\n",
      "|  57114|Друзья, давайте в...|1552309202|\n",
      "+-------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "posts_df = spark.read.json(\"shared-data/bigdata20/followers_posts_api_final.json\")\\\n",
    "    .select(col(\"owner_id\").alias(\"user_id\"), \"text\", col(\"date\").alias(\"timestamp\"))\\\n",
    "    .where(\"text != ''\")\n",
    "posts_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-------+---+----+\n",
      "|user_id|sex| age|\n",
      "+-------+---+----+\n",
      "|     34|  F|null|\n",
      "|    102|  F|null|\n",
      "|    175|  M|null|\n",
      "|    243|  F|  34|\n",
      "|    533|  M|  32|\n",
      "+-------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "profile_df = spark.read.json(\"shared-data/bigdata20/followers_info.json\").select(\n",
    "        col(\"id\").alias(\"user_id\"), (\n",
    "            when(col(\"sex\") == lit(1), lit(\"F\"))\n",
    "            .when(col(\"sex\") == lit(2), lit(\"M\"))\n",
    "            .otherwise(lit(None))\n",
    "        ).alias(\"sex\"),\n",
    "        (months_between(\n",
    "            current_date(), \n",
    "            concat_ws(\n",
    "                \"-\", regexp_extract(col(\"bdate\"), r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\", 3).cast(\"int\"), \n",
    "                regexp_extract(col(\"bdate\"), r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\", 2).cast(\"int\"), \n",
    "                regexp_extract(col(\"bdate\"), r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\", 1).cast(\"int\")\n",
    "            ).cast(\"date\")\n",
    "        ) / lit(12)).cast(\"int\").alias(\"age\")\n",
    "    )\n",
    "profile_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "+-------+---+----+--------------------+----------+\n",
      "|user_id|sex| age|                text| timestamp|\n",
      "+-------+---+----+--------------------+----------+\n",
      "|  39499|  M|  14|        #ЧайнаяЛента|1554622080|\n",
      "|  39499|  M|  14|Побывали в [club2...|1554742564|\n",
      "|  68686|  M|null|Полезный курс для...|1550587946|\n",
      "|  60086|  M|  34|С днем рождения! ...|1556793944|\n",
      "|  71307|  M|null|Женька, мой герой...|1552734777|\n",
      "+-------+---+----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "user_post_df = posts_df.join(broadcast(profile_df), [\"user_id\"])\\\n",
    "    .select(\"user_id\", \"sex\", \"age\", \"text\", \"timestamp\")\n",
    "user_post_df.printSchema()\n",
    "user_post_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "user_post_cnt = user_post_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "  0%|          | 81/163196 [01:25<53:15:02,  1.18s/it]"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=\"kafka-svc:9092\", value_serializer=str.encode)\n",
    "topic_name = \"main_topic\"\n",
    "\n",
    "for row in tqdm(user_post_df.orderBy(\"timestamp\").rdd.toLocalIterator(), total=user_post_cnt):\n",
    "    value = json.dumps(row.asDict(), ensure_ascii=False)\n",
    "    producer.send(topic_name, json.dumps(row.asDict(), ensure_ascii=False))\n",
    "    time.sleep(random.uniform(0.1, 2.))\n",
    "\n",
    "print(\"=\" * 80, \"\\nProducer finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}