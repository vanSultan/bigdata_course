{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if \"HADOOP_CONF_DIR\" in os.environ:\n",
    "   del os.environ[\"HADOOP_CONF_DIR\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "\"HADOOP_CONF_DIR\" in os.environ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Web UI: http://10.128.251.168:4040\n",
      "\n",
      "log4j file: /home/jovyan/conf/pyspark-log4j-jupsparkapp.properties\n",
      "\n",
      "driver log file: /home/jovyan/logs/pyspark-jupsparkapp.log\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import BooleanType, IntegerType, LongType, StringType, ArrayType, FloatType, StructType, StructField, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "\n",
    "# setting constants\n",
    "APP_NAME = \"jupsparkapp\"\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_')\n",
    "\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = \"-Dlog4j.configuration=file://{} -Dspark.hadoop.dfs.replication=1 -Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\\\n",
    "    .format(LOG4J_PROP_FILE)\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "# preparing configuration files from templates\n",
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template\\\n",
    "    .stream(logfile=LOG_FILE)\\\n",
    "    .dump(LOG4J_PROP_FILE)\n",
    "\n",
    "# run spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(APP_NAME)\\\n",
    "    .master(\"k8s://https://10.32.7.103:6443\")\\\n",
    "    .config(\"spark.driver.host\", LOCAL_IP)\\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", '3')\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.6\")\\\n",
    "    .config(\"spark.executor.memory\", '3g')\\\n",
    "    .config(\"spark.driver.memory\", \"3g\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "    .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\\\n",
    "    .config(\"spark.kubernetes.namespace\", \"szhumabaev-307617\")\\\n",
    "    .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.container.image\", \"node03.st:5000/spark-executor:szhumabaev-307617\")\\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.path\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.readOnly\", \"false\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.path\", \"/home/jovyan/shared-data\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.path\", \"/nfs/shared\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.type\", \"Directory\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.readOnly\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# printing important urls and paths\n",
    "print(\"Web UI: {}\".format(spark.sparkContext.uiWebUrl))\n",
    "print(\"\\nlog4j file: {}\".format(LOG4J_PROP_FILE))\n",
    "print(\"\\ndriver log file: {}\".format(LOG_FILE))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "schema = StructType()\\\n",
    "        .add(\"user_id\", LongType())\\\n",
    "        .add(\"sex\", StringType())\\\n",
    "        .add(\"age\", IntegerType())\\\n",
    "        .add(\"text\", StringType())\\\n",
    "        .add(\"timestamp\", TimestampType())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-svc:9092\")\\\n",
    "    .option(\"subscribe\", \"main_topic\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\\\n",
    "    .withColumn(\"age_group\", \n",
    "        when(col(\"age\").isNull() | (col(\"age\") < lit(18)), lit(\"0-18\"))\n",
    "        .when(col(\"age\").between(lit(18), lit(26)), lit(\"18-27\"))\n",
    "        .when(col(\"age\").between(lit(27), lit(39)), lit(\"27-40\"))\n",
    "        .when(col(\"age\").between(lit(40), lit(59)), lit(\"40-60\"))\n",
    "        .when(col(\"age\") >= 60, lit(\"60+\"))\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "import nltk\n",
    "nltk.data.path.append(\"/home/jovyan/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pattern_punct = \"[!@\\\"“’«»№#$%&'()*+\\.,\\-—/:;<=>?^_`{|}~\\[\\]\\d]\"\n",
    "pattern_url = \"http[s]?://\\S+|www\\.\\S+\"\n",
    "ru_stopwords = stopwords.words('russian')\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"cleaned\", outputCol=\"tokens\", pattern=r\"\\s+\")\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def preprocess_udf(tokens):\n",
    "    return [\n",
    "        \"\".join(c for c in word if c.isalnum()) \n",
    "        for word in tokens \n",
    "        if word not in ru_stopwords and word[0].isalnum()\n",
    "    ]\n",
    "\n",
    "def explode_words_ru(base_df):\n",
    "    return regexTokenizer.transform(\n",
    "            base_df.withColumn(\"cleaned\", regexp_replace(regexp_replace(\"text\", pattern_url, \" \"), pattern_punct, \" \"))\n",
    "        ).withColumn(\"finished\", preprocess_udf(\"tokens\"))\\\n",
    "        .withColumn(\"word\", explode(\"finished\"))\\\n",
    "        .drop(\"text\", \"cleaned\", \"tokens\", \"finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "def get_queries_list(base_df: DataFrame) -> list:\n",
    "    queries_list = list()\n",
    "    for sex in (\"F\", \"M\"):\n",
    "        for age_group in (\"0-18\", \"18-27\", \"27-40\", \"60+\"):\n",
    "            for win_dur in ((\"1 hour\", \"1h\"), (\"1 day\", \"1d\"), (\"1 week\", \"1w\")):\n",
    "                query = explode_words_ru(base_df)\\\n",
    "                    .where(f\"sex = '{sex}' and age_group = '{age_group}'\")\\\n",
    "                    .groupBy(window(\"timestamp\", win_dur[0]).alias(win_dur[1]), \"sex\", \"age_group\", \"word\").count()\\\n",
    "                    .writeStream\\\n",
    "                    .outputMode(\"update\")\\\n",
    "                    .format(\"console\")\\\n",
    "                    .option(\"truncate\", \"false\")\n",
    "                query_dict = dict(query=query, sex=sex, age_group=age_group, win_dur=win_dur[1])\n",
    "                queries_list.append(query_dict)\n",
    "    return queries_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "q_lst = get_queries_list(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# for q_d in [q for q in q_lst if q[\"sex\"] == \"F\" and q[\"age_group\"] == \"0-18\"]:\n",
    "for q_d in [q for q in q_lst]:\n",
    "    q_d[\"query_s\"] = q_d[\"query\"].start()\n",
    "    # q_d[\"query_s\"].stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}